{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendices\n",
    "\n",
    "Source code to replicate key results from the appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and imports\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import helpers\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision.models import resnet50\n",
    "import timeit\n",
    "import torch\n",
    "from spectral_sobol.torch_explainer import WaveletSobol\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of designs\n",
    "\n",
    "The number of designs corresponds to the number of deviations around the mean sequence. It is used to compute the variance of the Sobol indices. However, a high number of designs leads to an increased computational burden. \n",
    "\n",
    "We study the effect of the number of designs on the estimation of the Sobol indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up \n",
    "\n",
    "device = 'cuda:2'\n",
    "model = resnet50(pretrained = True).to(device)\n",
    "\n",
    "classes = {\n",
    " 'fox.png': 278,\n",
    " 'snow_fox.png': 279,\n",
    " 'polar_bear.png': 296,\n",
    " 'leopard.png': 288,\n",
    " 'fox1.jpg': 277,\n",
    " 'fox2.jpg': 277,\n",
    " 'sea_turtle.jpg': 33,\n",
    " 'lynx.jpg': 287,\n",
    " 'cat.jpg': 281,\n",
    " 'otter.jpg': 360\n",
    "}\n",
    "\n",
    "image_names = list(classes.keys())\n",
    "\n",
    "images = [Image.open('../assets/{}'.format(img_name)) for img_name in image_names]\n",
    "\n",
    "# transforms\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "preprocessing = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    " \n",
    "x = torch.stack(\n",
    "    [preprocessing(img) for img in images]\n",
    ")\n",
    "\n",
    "y = np.array([classes[img_name] for img_name in image_names])\n",
    "\n",
    "# loop with a varying number of designs\n",
    "\n",
    "variants = []\n",
    "times = []\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    print('Testing with i = {}'.format(i))\n",
    "    wavelet = WaveletSobol(model, grid_size = 28, nb_design = 2 ** (i+1), batch_size = 128)\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "    variants.append(wavelet(x,y))\n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    times.append(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "\n",
    "size = 224 # input size\n",
    "levels = 3 # number of levels in the wavelet decomposition\n",
    "\n",
    "indices = [3,4,7]\n",
    "\n",
    "n_rows = len(indices)\n",
    "n_cols = len(variants)\n",
    "\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize = (4 * n_cols, 4 * n_rows))\n",
    "\n",
    "for j in range(n_cols):\n",
    "\n",
    "    variant = variants[j] # retrieve the list of wcams\n",
    "    \n",
    "    for k, index in enumerate(indices) : # only plot some samples\n",
    "\n",
    "        nb_design = 2 ** (j+1)\n",
    "\n",
    "        title = 'Number of designs : {} \\n Computation time (s) : {:0.2f}'.format(nb_design, times[j] / 10)\n",
    "        \n",
    "        ax[0,j].set_title(title)\n",
    "        ax[k,j].imshow(variant[index], cmap = 'jet')\n",
    "        ax[k,j].axis('off')\n",
    "\n",
    "        helpers.add_lines(size, levels, ax[k,j])\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "\n",
    "plt.savefig('../figs/nb_design.pdf')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid size and number of levels\n",
    "\n",
    "Evaluate the effect of the grid size and the number of levels on the shape of the WCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency between Fourier-CAMs, WCAMs and existing literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs by the user\n",
    "n_samples = 100\n",
    "data_dir = \"../../data/ImageNet\"\n",
    "device = 'cuda:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ups\n",
    "\n",
    "# data a set of samples from IN validation set\n",
    "samples = [s for s in os.listdir(data_dir) if s[-5:] == '.JPEG']\n",
    "np.random.seed(42)\n",
    "samples = np.random.choice(samples, n_samples) # restrict the list of samples\n",
    "\n",
    "# load the labels dataframe\n",
    "labels = helpers.format_dataframe(data_dir, filter = samples)\n",
    "\n",
    "# transforms\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224)\n",
    "])\n",
    "\n",
    "normalize = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# load the images and generate the labels \n",
    "images = [preprocess(Image.open(os.path.join(data_dir, s)).convert('RGB')) for s in samples]\n",
    "y = np.array([ # generate the labels\n",
    "    labels[labels['name'] == s]['label'].values.item() for s in samples\n",
    "])\n",
    "\n",
    "# images passed to the model\n",
    "x = torch.stack([\n",
    "    normalize(im) for im in images\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model set up\n",
    "# model (and case)\n",
    "# model zoo features : \n",
    "#               - RT : 'augmix', 'pixmix', 'sin' (highest accuracy on ImageNet-C), \n",
    "#               - AT : 'adv_free, fast_adv and adv,\n",
    "#               - ST : standard training ('baseline')\n",
    "\n",
    "models_dir = '../../models/spectral-attribution-baselines'\n",
    "cases = ['baseline', 'augmix', 'pixmix', 'sin', 'adv_free', 'fast_adv', 'adv']\n",
    "\n",
    "# load the model\n",
    "\n",
    "models = []\n",
    "for case in cases:\n",
    "    \n",
    "    if case == 'baseline':\n",
    "        model = resnet50(pretrained = True).to(device).eval()\n",
    "    elif case in ['augmix', 'pixmix', 'sin']:\n",
    "        model = resnet50(pretrained = False) # model backbone #torch.load(os.path.join(models_dir, '{}.pth'.format(case))).eval()\n",
    "        weights = torch.load(os.path.join(models_dir, '{}.pth.tar').format(case))\n",
    "        model.load_state_dict(weights['state_dict'], strict = False)\n",
    "        model.eval()\n",
    "    elif case in ['adv_free', 'fast_adv', 'adv']:\n",
    "        model = resnet50(pretrained = False) # model backbone #torch.load(os.path.join(models_dir, '{}.pth'.format(case))).eval()\n",
    "        weights = torch.load(os.path.join(models_dir, \"{}.pth\".format(case)))\n",
    "        model.load_state_dict(weights)\n",
    "        model.eval()\n",
    "\n",
    "    # we will loop over the models\n",
    "    models.append(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency between Fourier-CAM and existing works\n",
    "\n",
    "We leverage the Fourier-CAM to measure the average frequency importance over the selected batch of images. \n",
    "\n",
    "#### Circular masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perturbation \n",
    "perturbation = 'circle' # other values are 'square' and 'grid' for the replication of \n",
    "                        # former paper's results \n",
    "grid_size = 14\n",
    "\n",
    "results = {}\n",
    "\n",
    "for case, model in zip(cases, models):\n",
    "    print('Case ........... {}'.format(case))\n",
    "    fourier = FourierSobol(model, grid_size = grid_size, nb_design = 16, batch_size=128, perturbation = perturbation)\n",
    "    _ = fourier(x,y)\n",
    "    results[case] = fourier.components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "# represent the importance of each frequency component in each case-\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# average the contributions per training mode\n",
    "average_baseline = np.sum(results['baseline'], axis = 0) / n_samples\n",
    "average_robust = np.sum(\n",
    "    [\n",
    "    np.sum(results[case], axis = 0) / n_samples for case in ['augmix', 'pixmix', 'sin']\n",
    "    ], axis = 0) / 3\n",
    "\n",
    "average_adversarial = np.sum(\n",
    "    [\n",
    "    np.sum(results[case], axis = 0) / n_samples for case in ['adv_free', 'fast_adv', \"adv\"]\n",
    "    ], axis = 0) / 3\n",
    "\n",
    "# average the contributions for each case\n",
    "average_contributions = {\n",
    "    'ST' : average_baseline,\n",
    "    'RT' : average_robust,\n",
    "    'AT' : average_adversarial\n",
    "    }\n",
    "\n",
    "# plots\n",
    "for i, case in enumerate(average_contributions.keys()):\n",
    "\n",
    "    average_contribution = average_contributions[case]\n",
    "\n",
    "    offset = 0.33 * i\n",
    "\n",
    "    plt.bar(np.array([*range(len(average_contribution))]) - 0.33 + offset , average_contribution, label = case, width = 0.2)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# plot labels\n",
    "plt.xlabel('Frequency component\\n (The higher the higher the frequency)')\n",
    "plt.ylabel('Importance (STI)')\n",
    "plt.title('Importance of the frequency components (circular case)')\n",
    "plt.xlim(-1,(grid_size // 2) + (grid_size // 4)) # resize as many circular components have no contribution\n",
    "plt.xticks([*range(0, (grid_size // 2) + (grid_size // 4))], rotation = 60)\n",
    "\n",
    "# plt.savefig(\"../figs/frequency-components-importance-complete.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Square masks\n",
    "\n",
    "Replication of the frequency components importance plot with square masks [(Zhang et al, 2022)](https://www.sciencedirect.com/science/article/abs/pii/S0925231222001084)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perturbation \n",
    "perturbation = 'square' # other values are 'square' and 'grid' for the replication of \n",
    "                        # former paper's results \n",
    "grid_size = 14\n",
    "results = {}\n",
    "\n",
    "for case, model in zip(cases, models):\n",
    "    print('Case ........... {}'.format(case))\n",
    "    fourier = FourierSobol(model, grid_size = grid_size, nb_design = 16, batch_size=128, perturbation = perturbation)\n",
    "    _ = fourier(x,y)\n",
    "    results[case] = fourier.components\n",
    "\n",
    "# plots\n",
    "# represent the importance of each frequency component in each case-\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# average the contributions per training mode\n",
    "average_baseline = np.sum(results['baseline'], axis = 0) / n_samples\n",
    "average_robust = np.sum(\n",
    "    [\n",
    "    np.sum(results[case], axis = 0) / n_samples for case in ['augmix', 'pixmix', 'sin']\n",
    "    ], axis = 0) / 3\n",
    "\n",
    "average_adversarial = np.sum(\n",
    "    [\n",
    "    np.sum(results[case], axis = 0) / n_samples for case in ['adv_free', 'fast_adv', \"adv\"]\n",
    "    ], axis = 0) / 3\n",
    "\n",
    "# average the contributions for each case\n",
    "average_contributions = {\n",
    "    'ST' : average_baseline,\n",
    "    'RT' : average_robust,\n",
    "    'AT' : average_adversarial\n",
    "    }\n",
    "\n",
    "# plots\n",
    "for i, case in enumerate(average_contributions.keys()):\n",
    "    average_contribution = average_contributions[case]\n",
    "\n",
    "    offset = 0.33 * i\n",
    "\n",
    "    plt.bar(np.array([*range(len(average_contribution))]) - 0.33 + offset , average_contribution, label = case, width = 0.25)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# plot labels\n",
    "plt.xlabel('Frequency component\\n (The higher the higher the frequency)')\n",
    "plt.ylabel('Importance (STI)')\n",
    "plt.title('Importance of the frequency components (square case)')\n",
    "plt.xticks([*range(0, grid_size)], rotation = 60)\n",
    "\n",
    "plt.savefig(\"../figs/frequency-components-importance-squares-complete.pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid masks (visualization of the importance in the nyquist Square)\n",
    "\n",
    "Replication of the same results but with grid masks [(Chen et al. 2022)](https://openreview.net/forum?id=rQ1cNbi07Vq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation = 'grid'\n",
    "grid_size = 17\n",
    "results = {}\n",
    "\n",
    "for case, model in zip(cases, models):\n",
    "    print('Case ............. {}'.format(case))\n",
    "    \n",
    "    fourier = FourierSobol(model, grid_size = grid_size, nb_design = 8, batch_size=128, perturbation = perturbation)\n",
    "    explanations = fourier(x,y)\n",
    "    results[case] = explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,7, figsize = (28, 4))\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "for i, case in enumerate(list(results.keys())):\n",
    "\n",
    "    mean = np.zeros((grid_size, grid_size))\n",
    "\n",
    "    count = 0\n",
    "    for map in results[case]:\n",
    "        if not np.isnan(map).any():\n",
    "\n",
    "            mean += cv2.resize(map, (grid_size, grid_size))\n",
    "            count += 1\n",
    "            \n",
    "    mean /= count\n",
    "    \n",
    "    ax[i].set_title('{}'.format(case))\n",
    "    im = ax[i].imshow(np.log(1 + mean), cmap = 'jet')\n",
    "\n",
    "    ax[i].set_xticks(([*range(grid_size)]))\n",
    "    ax[i].set_xticklabels(np.array([*range(grid_size)]) - (grid_size // 2))\n",
    "\n",
    "    ax[i].set_yticks(([*range(grid_size)]))\n",
    "    ax[i].set_yticklabels(np.array([*range(grid_size)]) - (grid_size // 2))\n",
    "\n",
    "    fig.colorbar(im, ax = ax[i], shrink = 0.8)\n",
    "plt.suptitle('Frequency importance in the Nyquist square for baseline, robust and adversarial training')\n",
    "fig.tight_layout()\n",
    "\n",
    "#plt.savefig('../figs/frequency-grid-complete.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency between WCAM and Fourier-CAM\n",
    "\n",
    "We show that we have similar results as in the previous plot with the WCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 28\n",
    "levels = 5\n",
    "opt = {\n",
    "    'approximation' : True,\n",
    "    'size' : grid_size\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for case, model in zip(cases, models):\n",
    "    print('Case ............... {}'.format(case))\n",
    "    wavelet = WaveletSobol(model, grid_size = grid_size, nb_design = 8, batch_size=128, opt = opt, levels = levels)\n",
    "    explanations = wavelet(x,y)\n",
    "    results[case] = explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the means\n",
    "levels = 5\n",
    "\n",
    "means = np.zeros((len(results.keys()), grid_size, grid_size))\n",
    "\n",
    "for i, case in enumerate(list(results.keys())):\n",
    "\n",
    "    if case == \"baseline\":\n",
    "        wcams = results[case][1:]\n",
    "    else:\n",
    "        wcams = results[case]\n",
    "\n",
    "    for wcam in wcams:\n",
    "        means[i] += cv2.resize(wcam, (grid_size, grid_size))\n",
    "\n",
    "    means[i] /= n_samples # average the values\n",
    "    means[0,0] = 0 # remove the 0th component (approximation coefficient)\n",
    "\n",
    "# work on the explanations to recover the frequencies \n",
    "averaged_coefficients = [helpers.reshape_wcam(means[i], grid_size, levels = levels) for i in range(means.shape[0])]\n",
    "\n",
    "# Reshape the contributions per ST, RT, AT\n",
    "baseline_coefficients = averaged_coefficients[0]\n",
    "robust_coefficients = np.mean(averaged_coefficients[1:4], axis = 0)\n",
    "adversarial_coefficients = np.mean(averaged_coefficients[-3:], axis = 0)\n",
    "\n",
    "# group again the coefficients\n",
    "averaged_coefficients = [baseline_coefficients, robust_coefficients, adversarial_coefficients]\n",
    "\n",
    "# plots\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "group_cases = ['ST', 'RT', 'AT']\n",
    "\n",
    "for i, (average_contribution, case) in enumerate(zip(averaged_coefficients, group_cases)):\n",
    "    offset = 0.33 * i\n",
    "    plt.bar(np.array([*range(len(average_contribution) - 1)]) - 0.33 + offset , average_contribution[1:] / sum(average_contribution[1:]), label = case, width = 0.2)\n",
    "\n",
    "# labels = [['a  \\n 0']]\n",
    "\n",
    "labels = []\n",
    "\n",
    "for level in range(levels):\n",
    "    labels.append(['h', 'd \\n{}'.format(level + 1), 'v'])\n",
    "labels = list(sum(labels, []))\n",
    "\n",
    "plt.xticks([*range(len(labels))], labels = labels)\n",
    "plt.title('Importance of the frequency components (WCAM case) \\n (without approximation coefficient)')\n",
    "plt.xlabel('Frequency/level')\n",
    "plt.ylabel('Importance (normalized STIs)')\n",
    "plt.legend()\n",
    "# plt.savefig('../figs/wcam-fcam-consistency-complete-no-approx.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acquisition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
