{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding how distribution shifts affect image classification models\n",
    "\n",
    "This notebooks displays images computed from the script `spectral_profile.py`. We display WCAMs obtained across various training methods and model backbones to show that the sensitivity to distribution shifts can be explained by a shift in the important regions for the prediction, as highlighted by the WCAM. Existing attribution methods cannot provide such qualitative evidence while robustness analyses have only shown that robust methods are quantitatively less subject to distribution shifts, but did not explicited whether it was because the model behave qualitatively differently. \n",
    "\n",
    "In this work, we provide evidence that the attitude of deep learining models with respect to the scale decomposition of an image is the same across (1) model baselines and (2) training approaches. Differences in robustness between those methods are only quantitative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# Libraries \n",
    "import os\n",
    "from utils import helpers\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mains plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models import resnet50\n",
    "from utils import corruptions\n",
    "from spectral_sobol.torch_explainer import WaveletSobol\n",
    "import torch\n",
    "\n",
    "device = 'cuda'\n",
    "source = '../assets'\n",
    "model = resnet50(pretrained = True).eval().to(device)\n",
    "batch_size = 128\n",
    "\n",
    "classes = { # dictionnary with the example images and labels\n",
    " 'fox.png': 278,\n",
    " 'snow_fox.png': 279,\n",
    " 'polar_bear.png': 296,\n",
    " 'leopard.png': 288,\n",
    " 'fox1.jpg': 277,\n",
    " 'fox2.jpg': 277,\n",
    " 'sea_turtle.jpg': 33,\n",
    " 'lynx.jpg': 287,\n",
    " 'cat.jpg': 281,\n",
    " 'otter.jpg': 360\n",
    "}\n",
    "\n",
    "# transforms\n",
    "\n",
    "# misc transforms\n",
    "resize_and_crop = torchvision.transforms.Compose([\n",
    "torchvision.transforms.Resize(256),\n",
    "torchvision.transforms.CenterCrop(224)\n",
    "])\n",
    "\n",
    "\n",
    "# transforms\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "\n",
    "preprocessing = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "# generate corrupted images\n",
    "# generate and resize the set of corrupted images\n",
    "# in this dictionnary, the 0th image is the source image (uncorrupted)\n",
    "corrupted_images = {\n",
    "    image_name : [resize_and_crop(im) for im in corruptions.generate_corruptions(os.path.join(source, image_name))] for image_name in classes.keys()\n",
    "}\n",
    "\n",
    "name = 'sea_turtle.jpg'\n",
    "images = corrupted_images[name]\n",
    "x = torch.stack([\n",
    "    preprocessing(im) for im in images\n",
    "])\n",
    "\n",
    "preds = helpers.evaluate_model_on_samples(x, model, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = corrupted_images[name]\n",
    "indices = [0, 3, 7]\n",
    "selected_images = [images[i] for i in indices]\n",
    "x = torch.stack([\n",
    "    preprocessing(im) for im in selected_images\n",
    "])\n",
    "\n",
    "wavelet = WaveletSobol(model, grid_size = 28, nb_design = 4, batch_size = 128, opt = {'approximation' : False})\n",
    "explanations = wavelet(x, np.array([33,33,33]).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize = (12,4))\n",
    "ax[0].imshow(explanations[0], cmap = 'jet')\n",
    "ax[1].imshow(explanations[1], cmap = 'jet')\n",
    "ax[2].imshow(explanations[2], cmap = 'jet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "imagenet_dir = '../../data/ImageNet/'\n",
    "\n",
    "classes_names = json.load(open(os.path.join(imagenet_dir,'classes-imagenet.json')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_names[str(preds[7].astype(int))].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2,3, figsize = (14,8))\n",
    "plt.rcParams.update({'font.size': 17})\n",
    "\n",
    "\n",
    "prediction = classes_names[str(preds[0].astype(int))].split(',')[0]\n",
    "ax[0,0].set_title('Prediction : {}'.format(prediction))\n",
    "ax[0,0].imshow(selected_images[0])\n",
    "\n",
    "prediction = classes_names[str(preds[3].astype(int))].split(',')[0]\n",
    "ax[0,1].set_title('Prediction : {}'.format(prediction))\n",
    "ax[0,1].imshow(selected_images[1])\n",
    "\n",
    "prediction = classes_names[str(preds[7].astype(int))].split(',')[0]\n",
    "ax[0,2].set_title('Prediction : {}'.format(prediction))\n",
    "ax[0,2].imshow(selected_images[2])\n",
    "\n",
    "\n",
    "ax[1,0].set_title('WCAM')\n",
    "ax[1,0].imshow(explanations[0], cmap = 'hot')\n",
    "helpers.add_lines(224, 3, ax[1,0])\n",
    "\n",
    "ax[1,1].set_title('WCAM')\n",
    "ax[1,1].imshow(explanations[1], cmap = 'hot')\n",
    "helpers.add_lines(224, 3, ax[1,1])\n",
    "\n",
    "ax[1,2].set_title('WCAM')\n",
    "ax[1,2].imshow(explanations[2], cmap = 'hot')\n",
    "helpers.add_lines(224, 3, ax[1,2])\n",
    "\n",
    "ax[0,0].axis('off')\n",
    "ax[0,1].axis('off')\n",
    "ax[0,2].axis('off')\n",
    "ax[1,0].axis('off')\n",
    "ax[1,1].axis('off')\n",
    "ax[1,2].axis('off')\n",
    "\n",
    "# plt.savefig('../figs/corruptions_baseline.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['sin', 'adv']\n",
    "\n",
    "models = [helpers.load_model(model_name, device) for model_name in model_names]\n",
    "\n",
    "corrupted_images = {\n",
    "    image_name : [resize_and_crop(im) for im in corruptions.generate_corruptions(os.path.join(source, image_name))] for image_name in classes.keys()\n",
    "}\n",
    "\n",
    "classes = { # dictionnary with the example images and labels\n",
    " 'fox.png': 278,\n",
    " 'snow_fox.png': 279,\n",
    " 'polar_bear.png': 296,\n",
    " 'leopard.png': 288,\n",
    " 'fox1.jpg': 277,\n",
    " 'fox2.jpg': 277,\n",
    " 'sea_turtle.jpg': 33,\n",
    " 'lynx.jpg': 287,\n",
    " 'cat.jpg': 281,\n",
    " 'otter.jpg': 360\n",
    "}\n",
    "\n",
    "name = 'fox.png'\n",
    "images = corrupted_images[name]\n",
    "x = torch.stack([\n",
    "    preprocessing(im) for im in images\n",
    "])\n",
    "\n",
    "preds = {model_name : helpers.evaluate_model_on_samples(x, model, batch_size) for model_name, model in zip(model_names, models)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "\n",
    "images = corrupted_images[name]\n",
    "selected_images = [images[i] for i in [0,8]]\n",
    "x = torch.stack([\n",
    "    preprocessing(im) for im in images\n",
    "])\n",
    "\n",
    "for model_name, model in zip(model_names, models):\n",
    "\n",
    "    wavelet = WaveletSobol(model, grid_size = 28, nb_design = 4, batch_size = 128, opt = {'approximation' : False})\n",
    "    \n",
    "    predictions = [278]\n",
    "    predictions.append(preds[model_name][8])\n",
    "    \n",
    "    explanations = wavelet(x, np.array(predictions).astype(np.uint8))\n",
    "\n",
    "    outputs[model_name] = explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3, figsize = (14,8))\n",
    "plt.rcParams.update({'font.size': 17})\n",
    "\n",
    "ax[0,0].imshow(selected_images[0])\n",
    "ax[0,0].axis('off')\n",
    "ax[0,0].set_title('Source image\\n Label : {}'.format(classes_names[str(278)]).split(',')[0])\n",
    "\n",
    "ax[0,1].set_title('SIN \\n Pred : {}'.format(classes_names[str(278)].split(',')[0]))\n",
    "ax[0,1].imshow(outputs['sin'][0], cmap = 'hot')\n",
    "helpers.add_lines(224, 3, ax[0,1])\n",
    "\n",
    "ax[1,1].set_title('Pred : {}'.format(classes_names[str(int(preds['sin'][8]))].split(',')[0]))\n",
    "ax[1,1].imshow(outputs['sin'][1], cmap = 'hot')\n",
    "helpers.add_lines(224, 3, ax[1,1])\n",
    "\n",
    "ax[0,2].set_title('Adv \\n Pred : {}'.format(classes_names[str(278)].split(',')[0]))\n",
    "ax[0,2].imshow(outputs['adv'][0], cmap = 'hot')\n",
    "helpers.add_lines(224, 3, ax[0,2])\n",
    "\n",
    "ax[1,2].set_title('Pred : {}'.format(classes_names[str(int(preds['adv'][8]))].split(',')[0]))\n",
    "ax[1,2].imshow(outputs['adv'][1], cmap = 'hot')\n",
    "helpers.add_lines(224, 3, ax[1,2])\n",
    "\n",
    "\n",
    "\n",
    "ax[0,1].axis('off')\n",
    "ax[0,2].axis('off')\n",
    "ax[1,1].axis('off')\n",
    "ax[1,2].axis('off')\n",
    "\n",
    "fig.delaxes(ax[1,0])\n",
    "plt.savefig('../figs/alternative-models.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data directory\n",
    "\n",
    "root = '../../data/spectral-attribution-outputs/'\n",
    "perturbations = ['corruptions']\n",
    "\n",
    "# get the name of the images to be plotted for each perturbation\n",
    "images = {\n",
    "    perturbation : os.listdir(os.path.join(root, perturbation)) for perturbation in perturbations\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below you can generate plots on the model of those depicted on figure 4 in the main paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = None \n",
    "perturbation = \"corruptions\"\n",
    "image_name = images[perturbation][0]\n",
    "device = 'cuda:1'\n",
    "\n",
    "case = 'baseline'\n",
    "labels = ['source', 'corrupted, unaffected', \"corrupted, affected\"]\n",
    "\n",
    "np.random.seed(95)\n",
    "directory = os.path.join(os.path.join(root, perturbation), image_name)\n",
    "\n",
    "\n",
    "model = helpers.load_model(case, device)\n",
    "helpers.plot_stable_and_unstable_prediction(image_name, model, directory, perturbation, case, labels, save = save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(95)\n",
    "\n",
    "# Loop \n",
    "\n",
    "# plot of the baseline case \n",
    "save = '../figs/figure-4/{}_{}_{}.pdf'\n",
    "perturbation = \"corruptions\"\n",
    "device = 'cuda:1'\n",
    "labels = ['source', 'corrupted, unaffected', \"corrupted, affected\"]\n",
    "\n",
    "\n",
    "cases = ['augmix']#, 'pixmix', \"adv\", \"adv_free\", 'fast_adv', \"sin\"]\n",
    "\n",
    "\n",
    "for case in cases:\n",
    "    print('Case ............. {}'.format(case))\n",
    "    \n",
    "    model = helpers.load_model(case, device)\n",
    "    for i, image_name in enumerate(images[perturbation]):\n",
    "        directory = os.path.join(os.path.join(root, perturbation), image_name)\n",
    "        helpers.plot_stable_and_unstable_prediction(image_name, model, directory, perturbation, case, labels, save = save.format(perturbation, case, i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below you can reproduce figures on the model of those depicted in figure 5 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the adversarial and SIN cases\n",
    "save = '../figs/figure-5/robust_training_corruptions_{}.pdf'\n",
    "cases = ['baseline', 'sin', 'augmix', \"pixmix\", \"adv\", \"adv_free\"]\n",
    "\n",
    "np.random.seed(95)\n",
    "\n",
    "for i, image_name in enumerate(images[perturbation]):\n",
    "    directory = os.path.join(os.path.join(root, perturbation), image_name)\n",
    "    helpers.plot_wcams(directory, cases, save = save.format(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acquisition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
