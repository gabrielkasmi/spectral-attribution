{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoom-in and accuracy: how much of the new information do models use?\n",
    "\n",
    "This notebook contains the source code to generate the figures provided in section 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and imports\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table generation\n",
    "\n",
    "Retrieve the values reported in table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from utils import helpers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/spectral-attribution-outputs'\n",
    "imagenet_path = '../../data/ImageNet/'\n",
    "filename = 'zoom_importance.json'\n",
    "\n",
    "results = json.load(open(os.path.join(data_path, filename)))\n",
    "images = results['images']\n",
    "\n",
    "cases = [k for k in results.keys() if not k == 'images']\n",
    "\n",
    "# for all cases, retrieve the tables and averages\n",
    "\n",
    "aggregate = np.zeros((5, 2 * len(cases)))\n",
    "stds = np.zeros((5, 2 * len(cases)))\n",
    "for i, case in enumerate(cases):\n",
    "    regular = np.mean(results[case]['regular'], axis = 1)\n",
    "    reg_std = np.std(results[case]['regular'], axis = 1)\n",
    "    zoomed = np.mean(results[case]['zoomed'], axis = 1)\n",
    "    zoom_std = np.std(results[case]['zoomed'], axis = 1)\n",
    "    \n",
    "    # add a 0 value for the 4th level of the regular wcam\n",
    "    regular = np.append(regular, 0.)\n",
    "    reg_std = np.append(reg_std, np.nan)\n",
    "\n",
    "    aggregate[:,2*i] = regular\n",
    "    aggregate[:,2*i+1] = zoomed\n",
    "\n",
    "    stds[:,2*i] = reg_std\n",
    "    stds[:,2*i+1] = zoom_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cases)\n",
    "print('Table coefficients') \n",
    "\n",
    "for i,(row, row_std) in enumerate(zip(aggregate, stds)):\n",
    "  \n",
    "    values = 'Level : {}   '.format(i)\n",
    "    values_std = '            '\n",
    "    for r, r_std in zip(row, row_std):\n",
    "        values = values + '{:0.3f}\\t'.format(r)\n",
    "        if np.isnan(r_std):\n",
    "            values_std = values_std + '(-)  \\t'\n",
    "        else :\n",
    "            values_std = values_std + '({:0.3f})\\t'.format(r_std)\n",
    "    values_std = values_std + '\\n'\n",
    "\n",
    "    print(values)\n",
    "    print(values_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustrations\n",
    "\n",
    "Consider an image, resize it and zoom in and resize it and consider it normal and see how the WCAM changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from spectral_sobol.torch_explainer import WaveletSobol\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters, images and explanations\n",
    "\n",
    "num_examples = 10 # number of examples to evaluate\n",
    "\n",
    "type = 'baseline' # model type\n",
    "device = 'multi' # device on which the model is sent\n",
    "images_dir = '../../data/ImageNet/'\n",
    "labels = helpers.format_dataframe(images_dir, images[:num_examples])\n",
    "\n",
    "grid_size = 32 # grid size and options\n",
    "opt = {'size' : grid_size}\n",
    "\n",
    "# model and explainer\n",
    "model = helpers.load_model(type, device)\n",
    "wavelet_3 = WaveletSobol(model, grid_size = grid_size, nb_design = 4, batch_size = 256, levels = 3, opt = opt)\n",
    "wavelet_4 = WaveletSobol(model, grid_size = grid_size, nb_design = 4, batch_size = 256, levels = 4, opt = opt)\n",
    "\n",
    "# load the images \n",
    "baseline_transforms = torchvision.transforms.Compose([\n",
    "torchvision.transforms.Resize(256),\n",
    "torchvision.transforms.CenterCrop(224)\n",
    "])\n",
    "\n",
    "zoomed_in = torchvision.transforms.Compose([\n",
    "torchvision.transforms.Resize(512),\n",
    "torchvision.transforms.CenterCrop(224)\n",
    "])\n",
    "\n",
    "normalize = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "# images and their label\n",
    "\n",
    "images_baseline = [\n",
    "    baseline_transforms(Image.open(os.path.join(images_dir,labels.iloc[i]['name'])).convert('RGB')) for i in range(labels.shape[0])\n",
    "]\n",
    "\n",
    "images_zoomed_in = [\n",
    "    zoomed_in(Image.open(os.path.join(images_dir,labels.iloc[i]['name'])).convert('RGB')) for i in range(labels.shape[0])\n",
    "]\n",
    "\n",
    "x_baseline = torch.stack([\n",
    "    normalize(im) for im in images_baseline\n",
    "])\n",
    "\n",
    "x_zoomed_in = torch.stack([\n",
    "    normalize(im) for im in images_zoomed_in\n",
    "])\n",
    "\n",
    "y = labels['label'].values.astype(np.uint8)\n",
    "\n",
    "# compute the explanations\n",
    "expl_baseline = wavelet_3(x_baseline,y)\n",
    "expl_zoomed_in = wavelet_4(x_zoomed_in,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 224\n",
    "\n",
    "# which example to plot\n",
    "index = 7\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize = (8,8))\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "ax[0,0].set_title('Regular image')\n",
    "\n",
    "ax[0,0].imshow(images_baseline[index])\n",
    "ax[0,0].axis('off')\n",
    "\n",
    "ax[0,1].set_title('Zoomed in image')\n",
    "\n",
    "ax[0,1].imshow(images_zoomed_in[index])\n",
    "ax[0,1].axis('off')\n",
    "\n",
    "ax[1,0].set_title('Regular WCAM')\n",
    "\n",
    "\n",
    "wcam_baseline = cv2.resize(expl_baseline[index], (224,224))\n",
    "ax[1,0].imshow(wcam_baseline, cmap = 'jet')\n",
    "ax[1,0].axis('off')\n",
    "helpers.add_lines(size, 3, ax[1,0])\n",
    "\n",
    "wcam_zoom = cv2.resize(expl_zoomed_in[index], (224,224))\n",
    "ax[1,1].imshow(wcam_zoom, cmap = 'jet')\n",
    "ax[1,1].axis('off')\n",
    "helpers.add_lines(size, 4, ax[1,1])\n",
    "ax[1,1].set_title('Zoomed-in WCAM')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('../figs/wcam_zoom_example.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acquisition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
