{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying the sufficient information for prediction\n",
    "\n",
    "Computation of the reconstruction depth and evaluation of this measure as a predictive measure for the robustness of a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and imports\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction depth and robustness\n",
    "\n",
    "We define the reconstruction depth as the number of coefficients necessary to reconstruct an image that is accurately predicted by the model. We wonder whether this metric correlates with the robustness of a prediction. We measure the robustness of the prediction as the ratio between images that are accurately predicted over the total number of perturbation. It encompasses two dimensions: the strenth of the corruption and the type of corruption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "data_dir = \"../../data/spectral-attribution-outputs\"\n",
    "\n",
    "cases = ['baseline', \"sin\", 'augmix', \"pixmix\", 'adv', 'adv_free', 'fast_adv', 'vit']\n",
    "perturbation = \"corruptions\"\n",
    "\n",
    "# ImageNet-C\n",
    "\n",
    "data_corruption = {}\n",
    "\n",
    "for case in cases:\n",
    "\n",
    "    file = json.load(open(os.path.join(data_dir, 'reconstruction_depth_{}_{}.json'.format(case, perturbation))))\n",
    "\n",
    "    data_corruption[case] = {\n",
    "        'robustness' : file['robustness'],\n",
    "        'reconstruction_depth' : {status : file[status] for status in ['corrupted', 'source']}\n",
    "    }\n",
    "\n",
    "\n",
    "# ImageNet-E\n",
    "data_editing = {}\n",
    "perturbation = \"editing\"\n",
    "\n",
    "for case in cases:\n",
    "\n",
    "    file = json.load(open(os.path.join(data_dir, 'reconstruction_depth_{}_{}.json'.format(case, perturbation))))\n",
    "\n",
    "    data_editing[case] = {\n",
    "        'robustness' : file['robustness'],\n",
    "        'reconstruction_depth' : {status : file[status] for status in ['corrupted', 'source']}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots : reconstruction depth v. robustness\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (12,6))\n",
    "\n",
    "# first plot the diagrams on ImageNet-C\n",
    "for case in cases:\n",
    "\n",
    "    robustness, reconstruction_depth = data_corruption[case]['robustness'], data_corruption[case]['reconstruction_depth']\n",
    "\n",
    "    no_nans_source = np.where(~np.isnan(reconstruction_depth['source']))[0]\n",
    "\n",
    "    # regression lines\n",
    "    m, b = np.polyfit(np.array(robustness)[no_nans_source], np.array(reconstruction_depth['source'])[no_nans_source]  / 784, 1)\n",
    "    ax[0].plot(np.array(robustness), m* np.array(robustness)+b, label = case)\n",
    "    ax[0].scatter(robustness, np.array(reconstruction_depth['source']) / 784, alpha = 0.1)\n",
    "\n",
    "# then on ImageNet-E\n",
    "for case in cases:\n",
    "\n",
    "    robustness, reconstruction_depth = data_editing[case]['robustness'], data_editing[case]['reconstruction_depth']\n",
    "\n",
    "    no_nans_source = np.where(~np.isnan(reconstruction_depth['source']))[0]\n",
    "\n",
    "    # regression lines\n",
    "    m, b = np.polyfit(np.array(robustness)[no_nans_source], np.array(reconstruction_depth['source'])[no_nans_source]  / 784, 1)\n",
    "    ax[1].plot(np.array(robustness), m* np.array(robustness)+b, label = case)\n",
    "\n",
    "    # scatter plot\n",
    "    ax[1].scatter(np.array(robustness)[no_nans_source], np.array(reconstruction_depth['source'])[no_nans_source] / 784, alpha = 0.1)\n",
    "\n",
    " # titles and plots layouts\n",
    "plt.suptitle('Correlation between robustness and reconstruction depth (RD)')\n",
    "\n",
    "ax[0].set_xlabel('Robustness to corruptions')\n",
    "ax[1].set_xlabel('Robustness to corruptions')\n",
    "\n",
    "ax[0].set_ylabel('Reconstruction depth')\n",
    "ax[1].set_ylabel('Reconstruction depth')\n",
    "\n",
    "ax[0].set_title('ImageNet-C')\n",
    "ax[1].set_title('ImageNet-E')\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "plt.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('../figs/reconstruction-depth.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Run the cells below to generate a few examples of the reconstruction and reconstruction depth on the images included in the `assets` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries needed for the examples\n",
    "from utils import helpers\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision.models import resnet50\n",
    "import torch\n",
    "import cv2\n",
    "from spectral_sobol.torch_explainer import WaveletSobol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folders\n",
    "imagenet_dir = \"../../data/ImageNet/\"\n",
    "\n",
    "# device and model\n",
    "device = 'cuda:1'\n",
    "model = resnet50(pretrained = True).to(device)\n",
    "\n",
    "# parameters\n",
    "grid_size = 28\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up \n",
    "classes = {\n",
    " 'fox.png': 278,\n",
    " 'snow_fox.png': 279,\n",
    " 'polar_bear.png': 296,\n",
    " 'leopard.png': 288,\n",
    " 'fox1.jpg': 277,\n",
    " 'fox2.jpg': 277,\n",
    " 'sea_turtle.jpg': 33,\n",
    " 'lynx.jpg': 287,\n",
    " 'cat.jpg': 281,\n",
    " 'otter.jpg': 360\n",
    "}\n",
    "\n",
    "image_names = list(classes.keys())\n",
    "\n",
    "images = [Image.open('../assets/{}'.format(img_name)) for img_name in image_names]\n",
    "\n",
    "# transforms\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "preprocessing = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    " \n",
    "x = torch.stack(\n",
    "    [preprocessing(img) for img in images]\n",
    ")\n",
    "\n",
    "y = np.array([classes[img_name] for img_name in image_names])\n",
    "\n",
    "# compute the explanations\n",
    "# downs\n",
    "wavelet = WaveletSobol(model, grid_size = grid_size, nb_design = 8, batch_size = batch_size, opt = {'size' : grid_size})\n",
    "explanations = wavelet(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the sets of perturbed images\n",
    "altered_images = {\n",
    "    name : helpers.reconstruct_images(image.cpu().permute(1,2,0), cam) for name, image, cam in zip(classes.keys(), x, explanations)\n",
    "}\n",
    "\n",
    "# inference on the altered images\n",
    "altered_inference = {}\n",
    "\n",
    "for image_name in altered_images.keys():\n",
    "\n",
    "    images = altered_images[image_name]\n",
    "\n",
    "    x = torch.stack(\n",
    "        [preprocessing(img) for img in images]  \n",
    "    )\n",
    "    altered_inference[image_name] = {\n",
    "        'preds' : helpers.evaluate_model_on_samples(x, model, batch_size),\n",
    "        'label' : classes[image_name]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some plots of the altered and reconstructed images\n",
    "\n",
    "fig, ax = plt.subplots(3,3, figsize = (12,12))\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "\n",
    "names = ['fox.png', 'polar_bear.png', 'lynx.jpg']\n",
    "\n",
    "for i,image_name in enumerate(names):\n",
    "\n",
    "    # original image (all coefficients)\n",
    "    ax[i,0].imshow(altered_images[image_name][-1])\n",
    "    ax[i,0].axis('off')\n",
    "    ax[i,1].imshow(altered_images[image_name][0])\n",
    "    ax[i,1].axis('off')\n",
    "    ax[i,2].imshow(altered_images[image_name][-2])\n",
    "    ax[i,2].axis('off')\n",
    "\n",
    "    n_pos = len(altered_images[image_name]) - 2\n",
    "\n",
    "    if i > 0:\n",
    "        ax[i,2].set_title('{} coefficients'.format(n_pos))\n",
    "        ax[i,2].axis('off')\n",
    "\n",
    "ax[0,0].set_title('Original image')\n",
    "ax[0,0].axis('off')\n",
    "ax[0,1].set_title('Higest coefficient')\n",
    "ax[0,1].axis('off')\n",
    "\n",
    "n_pos = len(altered_images[names[0]]) - 1\n",
    "ax[0,2].set_title('Positive coefficients only\\n {} coefficients'.format(n_pos))\n",
    "ax[0,2].axis('off')\n",
    "\n",
    "plt.suptitle('Reconstruction of images with a subset of Wavelet coefficients')\n",
    "# plt.savefig('../figs/reconstruction-examples.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some plots of the model' predictions for these images\n",
    "\n",
    "fig, ax = plt.subplots(2,4, figsize = (16,6))\n",
    "\n",
    "size = 224\n",
    "levels = 3\n",
    "\n",
    "# names = ['fox.png', 'polar_bear.png', 'lynx.jpg']\n",
    "names = ['cat.jpg', 'fox2.jpg']\n",
    "for i, name in enumerate(names):\n",
    "    \n",
    "    preds, label = altered_inference[name]['preds'], altered_inference[name]['label'] \n",
    "    depth = np.min(np.where(preds == label * np.ones(len(preds)))[0])\n",
    "    class_index = list(classes.keys()).index(name)\n",
    "\n",
    "\n",
    "    # plot the image\n",
    "    ax[i,0].imshow(altered_images[name][-1])\n",
    "    ax[i,0].axis('off')\n",
    "\n",
    "    # plot the minimal correctly predicted image\n",
    "    ax[i,1].imshow(altered_images[name][depth])\n",
    "    ax[i,1].axis('off')\n",
    "\n",
    "    # plot the spatial wcam as overlay\n",
    "    ax[i,2].imshow(altered_images[name][depth])\n",
    "    ax[i,2].imshow(wavelet.spatial_cam[class_index], cmap = 'jet', alpha = 0.5)\n",
    "    ax[i,2].axis('off')\n",
    "\n",
    "    # plot the wcam\n",
    "    class_index = list(classes.keys()).index(name)\n",
    "    wcam = cv2.resize(explanations[class_index], (size, size), interpolation = cv2.INTER_CUBIC)\n",
    "    ax[i,3].imshow(wcam, cmap = 'jet')\n",
    "    ax[i,3].axis('off')\n",
    "    helpers.add_lines(size, levels, ax[i,3])\n",
    "\n",
    "ax[0,0].set_title('Original image')\n",
    "ax[0,1].set_title('Sufficient image')\n",
    "ax[0,2].set_title('Sufficient image \\n and spatial WCAM')\n",
    "ax[0,3].set_title('WCAM')\n",
    "\n",
    "# plt.savefig('../figs/reconstruction-sufficient-2.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acquisition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
