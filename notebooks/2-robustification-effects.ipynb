{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustification effects\n",
    "\n",
    "In this notebook, we measure how the standard robustness enhancement techniques improve the robustness of the models. We also show that the properties highlighted for baseline models also holds for robust models. \n",
    "\n",
    "We consider three settings \n",
    "* ST (standard training) which has been studied in the first notebook\n",
    "* AT (adversarial training) we study how adversarial training affects the model reliance on frequencies\n",
    "* RT (robust training) we study how robust training affects the model reliance on frequencies\n",
    "\n",
    "Punchline: difference in degrees, not in nature. The remaining (yet less numerous) errors behave the same as for the ST model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# library imports\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from utils import helpers, corruptions\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision.models import resnet50\n",
    "from spectral_sobol.torch_explainer import FourierSobol, WaveletSobol\n",
    "from robustness.datasets import ImageNet\n",
    "from robustness.model_utils import make_and_restore_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs by the user\n",
    "n_samples = 100\n",
    "data_dir = \"../../data/ImageNet\"\n",
    "device = 'cuda:2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ups\n",
    "\n",
    "# data a set of samples from IN validation set\n",
    "samples = [s for s in os.listdir(data_dir) if s[-5:] == '.JPEG']\n",
    "np.random.seed(42)\n",
    "samples = np.random.choice(samples, n_samples) # restrict the list of samples\n",
    "\n",
    "# load the labels dataframe\n",
    "labels = helpers.format_dataframe(data_dir, filter = samples)\n",
    "\n",
    "# transforms\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224)\n",
    "])\n",
    "\n",
    "normalize = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# load the images and generate the labels \n",
    "images = [preprocess(Image.open(os.path.join(data_dir, s)).convert('RGB')) for s in samples]\n",
    "y = np.array([ # generate the labels\n",
    "    labels[labels['name'] == s]['label'].values.item() for s in samples\n",
    "])\n",
    "\n",
    "# images passed to the model\n",
    "x = torch.stack([\n",
    "    normalize(im) for im in images\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model set up\n",
    "# model (and case)\n",
    "# model zoo features : \n",
    "#               - RT : 'augmix', 'pixmix', 'sin' (highest accuracy on ImageNet-C), \n",
    "#               - AT : 'adv_free, fast_adv and adv,\n",
    "#               - ST : standard training ('baseline')\n",
    "\n",
    "models_dir = '../../models/spectral-attribution-baselines'\n",
    "cases = ['baseline', 'augmix', 'pixmix', 'sin', 'adv_free', 'fast_adv', 'adv']\n",
    "\n",
    "# load the model\n",
    "\n",
    "models = []\n",
    "for case in cases:\n",
    "    \n",
    "    if case == 'baseline':\n",
    "        model = resnet50(pretrained = True).to(device).eval()\n",
    "    elif case in ['augmix', 'pixmix', 'sin']:\n",
    "        model = resnet50(pretrained = False) # model backbone #torch.load(os.path.join(models_dir, '{}.pth'.format(case))).eval()\n",
    "        weights = torch.load(os.path.join(models_dir, '{}.pth.tar').format(case))\n",
    "        model.load_state_dict(weights['state_dict'], strict = False)\n",
    "        model.eval()\n",
    "    elif case in ['adv_free', 'fast_adv', 'adv']:\n",
    "        model = resnet50(pretrained = False) # model backbone #torch.load(os.path.join(models_dir, '{}.pth'.format(case))).eval()\n",
    "        weights = torch.load(os.path.join(models_dir, \"{}.pth\".format(case)))\n",
    "        model.load_state_dict(weights)\n",
    "        model.eval()\n",
    "\n",
    "    # we will loop over the models\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency importance\n",
    "\n",
    "We leverage the Fourier-CAM to measure the average frequency importance over the selected batch of images. \n",
    "\n",
    "### Importance of the frequency components using the circular masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perturbation \n",
    "perturbation = 'circle' # other values are 'square' and 'grid' for the replication of \n",
    "                        # former paper's results \n",
    "grid_size = 14\n",
    "\n",
    "results = {}\n",
    "\n",
    "for case, model in zip(cases, models):\n",
    "    print('Case ........... {}'.format(case))\n",
    "    fourier = FourierSobol(model, grid_size = grid_size, nb_design = 16, batch_size=128, perturbation = perturbation)\n",
    "    _ = fourier(x,y)\n",
    "    results[case] = fourier.components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "# represent the importance of each frequency component in each case-\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# average the contributions per training mode\n",
    "average_baseline = np.sum(results['baseline'], axis = 0) / n_samples\n",
    "average_robust = np.sum(\n",
    "    [\n",
    "    np.sum(results[case], axis = 0) / n_samples for case in ['augmix', 'pixmix', 'sin']\n",
    "    ], axis = 0) / 3\n",
    "\n",
    "average_adversarial = np.sum(\n",
    "    [\n",
    "    np.sum(results[case], axis = 0) / n_samples for case in ['adv_free', 'fast_adv', \"adv\"]\n",
    "    ], axis = 0) / 3\n",
    "\n",
    "# average the contributions for each case\n",
    "average_contributions = {\n",
    "    'ST' : average_baseline,\n",
    "    'RT' : average_robust,\n",
    "    'AT' : average_adversarial\n",
    "    }\n",
    "\n",
    "# plots\n",
    "for i, case in enumerate(average_contributions.keys()):\n",
    "\n",
    "    average_contribution = average_contributions[case]\n",
    "\n",
    "    offset = 0.33 * i\n",
    "\n",
    "    plt.bar(np.array([*range(len(average_contribution))]) - 0.33 + offset , average_contribution, label = case, width = 0.2)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# plot labels\n",
    "plt.xlabel('Frequency component\\n (The higher the higher the frequency)')\n",
    "plt.ylabel('Importance (STI)')\n",
    "plt.title('Importance of the frequency components (circular case)')\n",
    "plt.xlim(-1,(grid_size // 2) + (grid_size // 4)) # resize as many circular components have no contribution\n",
    "plt.xticks([*range(0, (grid_size // 2) + (grid_size // 4))], rotation = 60)\n",
    "\n",
    "# plt.savefig(\"../figs/frequency-components-importance-complete.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WCAM frequency importance\n",
    "\n",
    "We show that we have similar results as in the previous plot with the WCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 28\n",
    "levels = 5\n",
    "opt = {\n",
    "    'approximation' : True,\n",
    "    'size' : grid_size\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for case, model in zip(cases, models):\n",
    "    print('Case ............... {}'.format(case))\n",
    "    wavelet = WaveletSobol(model, grid_size = grid_size, nb_design = 8, batch_size=128, opt = opt, levels = levels)\n",
    "    explanations = wavelet(x,y)\n",
    "    results[case] = explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the means\n",
    "levels = 5\n",
    "\n",
    "means = np.zeros((len(results.keys()), grid_size, grid_size))\n",
    "\n",
    "for i, case in enumerate(list(results.keys())):\n",
    "\n",
    "    if case == \"baseline\":\n",
    "        wcams = results[case][1:]\n",
    "    else:\n",
    "        wcams = results[case]\n",
    "\n",
    "    for wcam in wcams:\n",
    "        means[i] += cv2.resize(wcam, (grid_size, grid_size))\n",
    "\n",
    "    means[i] /= n_samples # average the values\n",
    "    means[0,0] = 0 # remove the 0th component (approximation coefficient)\n",
    "\n",
    "# work on the explanations to recover the frequencies \n",
    "averaged_coefficients = [helpers.reshape_wcam(means[i], grid_size, levels = levels) for i in range(means.shape[0])]\n",
    "\n",
    "# Reshape the contributions per ST, RT, AT\n",
    "baseline_coefficients = averaged_coefficients[0]\n",
    "robust_coefficients = np.mean(averaged_coefficients[1:4], axis = 0)\n",
    "adversarial_coefficients = np.mean(averaged_coefficients[-3:], axis = 0)\n",
    "\n",
    "# group again the coefficients\n",
    "averaged_coefficients = [baseline_coefficients, robust_coefficients, adversarial_coefficients]\n",
    "\n",
    "# plots\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "group_cases = ['ST', 'RT', 'AT']\n",
    "\n",
    "for i, (average_contribution, case) in enumerate(zip(averaged_coefficients, group_cases)):\n",
    "    offset = 0.33 * i\n",
    "    plt.bar(np.array([*range(len(average_contribution) - 1)]) - 0.33 + offset , average_contribution[1:] / sum(average_contribution[1:]), label = case, width = 0.2)\n",
    "\n",
    "# labels = [['a  \\n 0']]\n",
    "\n",
    "labels = []\n",
    "\n",
    "for level in range(levels):\n",
    "    labels.append(['h', 'd \\n{}'.format(level + 1), 'v'])\n",
    "labels = list(sum(labels, []))\n",
    "\n",
    "plt.xticks([*range(len(labels))], labels = labels)\n",
    "plt.title('Importance of the frequency components (WCAM case) \\n (without approximation coefficient)')\n",
    "plt.xlabel('Frequency/level')\n",
    "plt.ylabel('Importance (normalized STIs)')\n",
    "plt.legend()\n",
    "plt.savefig('../figs/wcam-fcam-consistency-complete-no-approx.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance-based behavior is the same across models\n",
    "\n",
    "We partially replicate the results from the first notebook to show that even for robust models, sensitivity to shifts translates into associating an image with the wrong spectral profile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common parameters for this section\n",
    "\n",
    "index = 2\n",
    "model = models[index]\n",
    "case = cases[index]\n",
    "n = 3 # number of images that will be considered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate corrupted images for a set of instances\n",
    "np.random.seed(42)\n",
    "image_names = np.random.choice(samples, n)\n",
    "\n",
    "corrupted_images = {\n",
    "    image_name : [preprocess(im) for im in corruptions.generate_corruptions(os.path.join(data_dir, image_name))] for image_name in image_names\n",
    "}\n",
    "\n",
    "# evaluate the model on the corrupted images\n",
    "batch_size = 128\n",
    "labels = helpers.format_dataframe(data_dir)\n",
    "\n",
    "model = models[2]\n",
    "\n",
    "results = {}\n",
    "for image_name in corrupted_images.keys():\n",
    "\n",
    "    label = labels[labels['name'] == image_name]['label'].values.item()\n",
    "    sequence = corrupted_images[image_name]\n",
    "\n",
    "    x = torch.stack(\n",
    "        [normalize(im) for im in sequence]\n",
    "    )\n",
    "\n",
    "    preds = helpers.evaluate_model_on_samples(x, model, batch_size)\n",
    "    results[image_name] = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute spectral profiles on the fly\n",
    "\n",
    "classes = {}\n",
    "n_items = 10\n",
    "grid_size = 28\n",
    "\n",
    "opt = {\n",
    "    'approximation' : False,\n",
    "    'size' : grid_size\n",
    "}\n",
    "\n",
    "# explainer\n",
    "wavelet = WaveletSobol(model, grid_size = grid_size, nb_design = 4, opt = opt)\n",
    "\n",
    "# parameters passed to the comoutation of the spectral profile\n",
    "params = {\n",
    "    'source_dir' : data_dir,\n",
    "    'preprocessing' : preprocess,\n",
    "    'normalize' : normalize,\n",
    "    \"explainer\" : wavelet\n",
    "\n",
    "}\n",
    "\n",
    "for image in results.keys():\n",
    "    \n",
    "    label = preds[0]\n",
    "    # get images that have the same label\n",
    "\n",
    "    preds = results[image]\n",
    "    shifted_labels = np.unique(preds[np.where(preds != label)[0]])\n",
    "\n",
    "    # compute the WCAM of the predicted label\n",
    "    if label not in classes.keys():\n",
    "        np.random.seed(42)\n",
    "        item_names = np.random.choice( # choose the items of interest\n",
    "            [l for l in labels[labels['label'] == label]['name'].values if not l == image], \n",
    "                                      size = n_items)\n",
    "\n",
    "        classes[label] = helpers.compute_spectral_profile(item_names, label, params)\n",
    "\n",
    "    # compute the WCAM of the shifted labels\n",
    "    for p in shifted_labels:\n",
    "        if p not in classes.keys():\n",
    "            np.random.seed(42)\n",
    "            item_names = np.random.choice( # choose the items of interest\n",
    "            [l for l in labels[labels['label'] == label]['name'].values if not l == image], \n",
    "                                      size = n_items)\n",
    "            classes[p] = helpers.compute_spectral_profile(item_names, p, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the wcams for the images\n",
    "wcams = {}\n",
    "\n",
    "for image_name in corrupted_images.keys():\n",
    "    images = corrupted_images[image_name]\n",
    "    x = torch.stack([\n",
    "        normalize(im) for im in images\n",
    "    ])\n",
    "\n",
    "    y = results[image_name].astype(np.uint8)\n",
    "\n",
    "    # compute the wcams\n",
    "    wcams[image_name] = wavelet(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot these profiles and the image-based wcams\n",
    "plt.rcParams.update({'font.size': 17})\n",
    "input_shape = (224,224)\n",
    "\n",
    "images_list = list(wcams.keys())\n",
    "index = 2\n",
    "\n",
    "image_name = images_list[index]\n",
    "\n",
    "\n",
    "# get the predictions \n",
    "preds = results[images_list[index]]\n",
    "img_wcams = wcams[image_name]\n",
    "\n",
    "source_images = corrupted_images[image_name]\n",
    "\n",
    "print(preds)\n",
    "\n",
    "\n",
    "# get the original image\n",
    "\n",
    "fig, ax = plt.subplots(2,3, figsize = (15,8))\n",
    "label = preds[0]\n",
    "\n",
    "ax[0,0].set_title('Original image')\n",
    "ax[0,0].imshow(source_images[0])\n",
    "ax[0,0].axis('off')\n",
    "\n",
    "# true class, wcam and class wcam\n",
    "ax[0,1].set_title('Image WCAM - class : {}'.format(preds[0].astype(int)))\n",
    "ax[0,1].imshow(cv2.resize(img_wcams[0], input_shape), cmap = 'jet')\n",
    "ax[0,2].set_title('Class average WCAM')\n",
    "ax[0,2].imshow(cv2.resize(classes[label], input_shape), cmap = 'jet')\n",
    "ax[0,1].axis('off')\n",
    "ax[0,2].axis('off')\n",
    "helpers.add_lines(224, 3, ax[0,2])\n",
    "helpers.add_lines(224, 3, ax[0,1])\n",
    "\n",
    "# true class, wcam and class wcam\n",
    "k = 9\n",
    "\n",
    "label = preds[k]\n",
    "\n",
    "ax[1,0].set_title('Corrupted image')\n",
    "ax[1,0].imshow(source_images[k])\n",
    "ax[1,0].axis('off')\n",
    "\n",
    "ax[1,1].set_title('Image WCAM - class : {}'.format(preds[k].astype(int)))\n",
    "ax[1,1].imshow(cv2.resize(img_wcams[k], input_shape), cmap = 'jet')\n",
    "ax[1,2].set_title('Class average WCAM')\n",
    "ax[1,2].imshow(cv2.resize(classes[label], input_shape), cmap = 'jet')\n",
    "helpers.add_lines(224, 3, ax[1,2])\n",
    "helpers.add_lines(224, 3, ax[1,1])\n",
    "ax[1,1].axis('off')\n",
    "ax[1,2].axis('off')\n",
    "\n",
    "plt.suptitle('WCAM for the original and corrupted image. Training setup : {}'.format(case))\n",
    "plt.savefig('../figs/class-swap-example-{}.pdf'.format(case))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices\n",
    "\n",
    "### Square masks\n",
    "\n",
    "Replication of the frequency components importance plot with square masks [(Zhang et al, 2022)](https://www.sciencedirect.com/science/article/abs/pii/S0925231222001084)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perturbation \n",
    "perturbation = 'square' # other values are 'square' and 'grid' for the replication of \n",
    "                        # former paper's results \n",
    "grid_size = 14\n",
    "results = {}\n",
    "\n",
    "for case, model in zip(cases, models):\n",
    "    print('Case ........... {}'.format(case))\n",
    "    fourier = FourierSobol(model, grid_size = grid_size, nb_design = 16, batch_size=128, perturbation = perturbation)\n",
    "    _ = fourier(x,y)\n",
    "    results[case] = fourier.components\n",
    "\n",
    "# plots\n",
    "# represent the importance of each frequency component in each case-\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# average the contributions per training mode\n",
    "average_baseline = np.sum(results['baseline'], axis = 0) / n_samples\n",
    "average_robust = np.sum(\n",
    "    [\n",
    "    np.sum(results[case], axis = 0) / n_samples for case in ['augmix', 'pixmix', 'sin']\n",
    "    ], axis = 0) / 3\n",
    "\n",
    "average_adversarial = np.sum(\n",
    "    [\n",
    "    np.sum(results[case], axis = 0) / n_samples for case in ['adv_free', 'fast_adv', \"adv\"]\n",
    "    ], axis = 0) / 3\n",
    "\n",
    "# average the contributions for each case\n",
    "average_contributions = {\n",
    "    'ST' : average_baseline,\n",
    "    'RT' : average_robust,\n",
    "    'AT' : average_adversarial\n",
    "    }\n",
    "\n",
    "# plots\n",
    "for i, case in enumerate(average_contributions.keys()):\n",
    "    average_contribution = average_contributions[case]\n",
    "\n",
    "    offset = 0.33 * i\n",
    "\n",
    "    plt.bar(np.array([*range(len(average_contribution))]) - 0.33 + offset , average_contribution, label = case, width = 0.25)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# plot labels\n",
    "plt.xlabel('Frequency component\\n (The higher the higher the frequency)')\n",
    "plt.ylabel('Importance (STI)')\n",
    "plt.title('Importance of the frequency components (square case)')\n",
    "plt.xticks([*range(0, grid_size)], rotation = 60)\n",
    "\n",
    "plt.savefig(\"../figs/frequency-components-importance-squares-complete.pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the importance in the Nyquist square\n",
    "\n",
    "Replication of the same results but with grid masks [(Chen et al. 2022)](https://openreview.net/forum?id=rQ1cNbi07Vq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation = 'grid'\n",
    "grid_size = 17\n",
    "results = {}\n",
    "\n",
    "for case, model in zip(cases, models):\n",
    "    print('Case ............. {}'.format(case))\n",
    "    \n",
    "    fourier = FourierSobol(model, grid_size = grid_size, nb_design = 8, batch_size=128, perturbation = perturbation)\n",
    "    explanations = fourier(x,y)\n",
    "    results[case] = explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,7, figsize = (28, 4))\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "for i, case in enumerate(list(results.keys())):\n",
    "\n",
    "    mean = np.zeros((grid_size, grid_size))\n",
    "\n",
    "    count = 0\n",
    "    for map in results[case]:\n",
    "        if not np.isnan(map).any():\n",
    "\n",
    "            mean += cv2.resize(map, (grid_size, grid_size))\n",
    "            count += 1\n",
    "            \n",
    "    mean /= count\n",
    "    \n",
    "    ax[i].set_title('{}'.format(case))\n",
    "    im = ax[i].imshow(np.log(1 + mean), cmap = 'jet')\n",
    "\n",
    "    ax[i].set_xticks(([*range(grid_size)]))\n",
    "    ax[i].set_xticklabels(np.array([*range(grid_size)]) - (grid_size // 2))\n",
    "\n",
    "    ax[i].set_yticks(([*range(grid_size)]))\n",
    "    ax[i].set_yticklabels(np.array([*range(grid_size)]) - (grid_size // 2))\n",
    "\n",
    "    fig.colorbar(im, ax = ax[i], shrink = 0.8)\n",
    "plt.suptitle('Frequency importance in the Nyquist square for baseline, robust and adversarial training')\n",
    "fig.tight_layout()\n",
    "\n",
    "#plt.savefig('../figs/frequency-grid-complete.pdf')\n",
    "plt.show()\n",
    "\n",
    "# TODO. With 7 plots (1 for each type of training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
