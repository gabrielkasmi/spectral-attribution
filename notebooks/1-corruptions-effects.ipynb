{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effects of natural image corruptions on a model's prediction\n",
    "\n",
    "This notebooks allows the replication of the results of the section 4.1. of the paper. We show that across backbones, under standard training, a corruption affects the salient scales (i.e. the important regions in the space-scale representation). In turn, the image is not longer matched with its class's typical scale profile, but rather with the scale profile of another class, resulting in a missclassification. \n",
    "\n",
    "We firs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "from utils import helpers, corruptions\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up: model backbone, and source data\n",
    "\n",
    "The requirements is to have a set of WCAMs precomputed from ImageNet validation set. These WCAMs can be computed from the script `generate_wcams.py`. When necessary, model weights are accessible on the public repository attached with this article. Backbones include the following:\n",
    "- CNN backbones: ResNet50, VGG16 (weights: taken from `torchvision`)\n",
    "- Transformer backbones: ViT-B16 and ViT-B32 (weights: taken from `torchvision`)\n",
    "- Self-Supervised backbones: Dino (ViT-B16, weights: taken from `torch hub`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download our folder from the Zenodo repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "backbone = \"resnet\" # backbones : 'resnet', 'vgg', 'vitb16', 'vitb32', 'dino'\n",
    "root_dir = '../../data/wcams-imagenet' # root: where wcams are stored\n",
    "imagenet_dir = '../../data/ImageNet/' # imagenet : where iamgenet is located\n",
    "\n",
    "wcams_dir = os.path.join(root_dir, backbone) # folder containing the wcams of the corresponding backbone\n",
    "                                                         # for each backbone, 5000 wcams were computed.\n",
    "labels_dir = os.path.join(imagenet_dir, \"val.txt\") # txt file with the labels of the imagenet validation set\n",
    "\n",
    "# create a dataframe with the labels of each image included in the wcam directory\n",
    "labels_raw = open(labels_dir).read().split('\\n')\n",
    "labels_dict = {r[:28] : int(r[29:]) for r in labels_raw if r[:10] == 'ILSVRC2012'}\n",
    "labels_true = pd.DataFrame.from_dict(labels_dict, orient = \"index\").reset_index()\n",
    "labels_true.columns = ['name', 'label']\n",
    "\n",
    "targets = os.listdir(wcams_dir) # list of images\n",
    "labels_wcam = labels_true[labels_true['name'].isin(targets)] # keep only the labels of the images\n",
    "\n",
    "# auxiliary file: the name of the imagenet classes \n",
    "classes_names = json.load(open(os.path.join(imagenet_dir,'classes-imagenet.json')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the class-wise scale profiles\n",
    "\n",
    "For each class, we compute the scale profiles. These profiles are obtained by averaging the WCAMS computed for each class. We then plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation\n",
    "# for the example, we plot the average WCAMs for 16 classes\n",
    "num_classes = 16\n",
    "\n",
    "classes = helpers.compute_average_classes(labels_wcam, wcams_dir)\n",
    "\n",
    "# plot\n",
    "\n",
    "fig, ax = plt.subplots(4,4, figsize = (16,16))\n",
    "input_shape = (224,224)\n",
    "\n",
    "np.random.seed(42)\n",
    "examples = np.random.choice(list(classes.keys()), num_classes, replace = False)\n",
    "\n",
    "row = -1\n",
    "\n",
    "for i, e in enumerate(examples):\n",
    "    \n",
    "    if i % 4 == 0:\n",
    "        row += 1\n",
    "        \n",
    "    label_class = classes_names[str(e)].split(',')\n",
    "        \n",
    "    ax[row, i%4].set_title('Class : {} ({})'.format(label_class[0], e))\n",
    "    \n",
    "    # upsample the WCAMs for better visualization\n",
    "    # change the colorbar \n",
    "    upscaled = cv2.resize(classes[e], input_shape)\n",
    "    im = ax[row, i%4].imshow(upscaled, cmap = 'jet')\n",
    "    helpers.add_lines(input_shape[0], 3, ax[row, i%4])\n",
    "\n",
    "    fig.colorbar(im, ax=ax[row, i%4], shrink=0.8)\n",
    "\n",
    "plt.suptitle('Average wavelet attributions for various ImageNet classes')\n",
    "fig.tight_layout()\n",
    "# plt.savefig('figs/classes-wcams.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the WCAMs of corrupted images\n",
    "\n",
    "Now that we have the average WCAM for the dataset classes, we consider a set of held out samples (i.e., coming from the remaining validation images). We apply a corruption to these samples and compute the WCAM across the set of corruptions. We then plot the WCAM of the predicted class (retrieved from the previous section) and the WCAM of the (corrupted) image. We show that the prediction is consistent with the pattern on the WCAM of the target class. \n",
    "\n",
    "If the model incorrectly classifies an image as belonging to class 42 while its true class is 999, then the WCAM will look like the average WCAM of the class 42. In particular, the <i> salient </i> scales are similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of corrupted images\n",
    "\n",
    "# regenerate the dataframe\n",
    "complete_labels = pd.DataFrame.from_dict(items, orient = 'index').reset_index()\n",
    "complete_labels.columns = ['name', 'label']\n",
    "\n",
    "# restrict to the the remaining samples \n",
    "# and to the targeted labels\n",
    "complete_labels = complete_labels[~complete_labels['name'].isin(targets)]\n",
    "complete_labels = complete_labels[complete_labels['label'].isin(examples)]\n",
    "\n",
    "sample_size = 50\n",
    "np.random.seed(42)\n",
    "example_images = np.random.choice(complete_labels['name'].values, sample_size)\n",
    "\n",
    "# generate the corruptions for these images\n",
    "corrupted_images = [corruptions.generate_corruptions(os.path.join(source_dir,im)) for im in example_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference \n",
    "device = \"cuda:2\"\n",
    "model = resnet50(pretrained = True).to(device)\n",
    "\n",
    "\n",
    "# transforms\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "preprocessing = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "results = {}\n",
    "\n",
    "for image, corruptions in zip(example_images, corrupted_images):\n",
    "\n",
    "    label = complete_labels[complete_labels['name'] == image]['label'].values.item()\n",
    "    x = torch.stack([\n",
    "        preprocessing(im.convert('RGB')) for im in corruptions\n",
    "    ]).to(device)\n",
    "    \n",
    "    preds = model(x).cpu().detach().numpy()\n",
    "    preds = np.argmax(preds, axis = 1)\n",
    "    \n",
    "    \n",
    "    if label == preds[0]:\n",
    "        results[image] = (label, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the WCAM for the selected set of images\n",
    "\n",
    "batch_size = 128\n",
    "grid_size = 28\n",
    "nb_design = 2\n",
    "opt = {\"approximation\" : False, 'size' : grid_size}\n",
    "\n",
    "wavelet = WaveletSobol(model, grid_size = grid_size, nb_design = nb_design, batch_size = batch_size, opt = opt)\n",
    "\n",
    "corruption_indices = [np.where(example_images == image)[0].item() for image in results.keys()]\n",
    "\n",
    "wcams = {}\n",
    "\n",
    "for image, index in zip(results.keys(), corruption_indices):\n",
    "        \n",
    "    images = corrupted_images[index]\n",
    "    x = torch.stack([\n",
    "        preprocessing(im.convert('RGB')) for im in images        \n",
    "    ]).to(device)\n",
    "    \n",
    "    y = np.array(results[image][1])\n",
    "    \n",
    "    wcams[image] = wavelet(x,y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list = list(wcams.keys())\n",
    "index = 0\n",
    "\n",
    "plt_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "])\n",
    "\n",
    "\n",
    "# get the predictions \n",
    "preds = results[images_list[index]][1]\n",
    "img_wcams = wcams[images_list[index]]\n",
    "\n",
    "source_images = corrupted_images[index]\n",
    "\n",
    "class_wcams = []\n",
    "\n",
    "for p in preds:\n",
    "    if p not in classes.keys():\n",
    "        class_wcams.append(np.nan * np.ones((28,28)))\n",
    "    else:\n",
    "        class_wcams.append(classes[p])\n",
    "\n",
    "# get the original image\n",
    "\n",
    "fig, ax = plt.subplots(2,3, figsize = (16, 8))\n",
    "\n",
    "ax[0,0].set_title('Original image')\n",
    "ax[0,0].imshow(plt_transform(source_images[0]))\n",
    "\n",
    "# true class, wcam and class wcam\n",
    "ax[0,1].set_title('Image WCAM - class : {}'.format(preds[0]))\n",
    "ax[0,1].imshow(cv2.resize(img_wcams[0], input_shape), cmap = 'jet')\n",
    "ax[0,2].set_title('Class average WCAM')\n",
    "ax[0,2].imshow(cv2.resize(class_wcams[0], input_shape), cmap = 'jet')\n",
    "utils.add_lines(224, 3, ax[0,2])\n",
    "utils.add_lines(224, 3, ax[0,1])\n",
    "\n",
    "# true class, wcam and class wcam\n",
    "k = 2\n",
    "\n",
    "ax[1,0].set_title('Corrupted image')\n",
    "ax[1,0].imshow(plt_transform(source_images[k]))\n",
    "\n",
    "ax[1,1].set_title('Image WCAM - class : {}'.format(preds[k]))\n",
    "ax[1,1].imshow(cv2.resize(img_wcams[k], input_shape), cmap = 'jet')\n",
    "ax[1,2].set_title('Class average WCAM')\n",
    "ax[1,2].imshow(cv2.resize(class_wcams[k], input_shape), cmap = 'jet')\n",
    "utils.add_lines(224, 3, ax[1,2])\n",
    "utils.add_lines(224, 3, ax[1,1])\n",
    "\n",
    "plt.suptitle('WCAM for the original and corrupted image')\n",
    "plt.savefig('figs/class-swap-example-3.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acquisition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dabb6a291954a723b19335c1c976d9e094c0e61cfafba1018bdf33ab55445e2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
